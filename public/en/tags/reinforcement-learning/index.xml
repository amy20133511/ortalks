<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning on ORAI China</title><link>http://localhost:1313/en/tags/reinforcement-learning/</link><description>Recent content in Reinforcement Learning on ORAI China</description><generator>Hugo 0.125.0</generator><language>en</language><copyright>运筹OR帷幄</copyright><lastBuildDate>Sun, 03 Sep 2023 10:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/en/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>[Tech Talk #41] Columbia DRO Tianyi Peng: When A/B Testing Platforms Meet Reinforcement Learning</title><link>http://localhost:1313/en/techtalk/techtalk_41/</link><pubDate>Sun, 03 Sep 2023 10:00:00 +0000</pubDate><guid>http://localhost:1313/en/techtalk/techtalk_41/</guid><description>A novel &amp;ldquo;Difference-in-Q&amp;rdquo; (DQ) estimator, based on reinforcement learning, is proposed to address the Interference problem in A/B testing. DQ outperforms traditional estimators in bias-variance trade-off, reducing bias and exponentially decreasing variance. Collaborating with ByteDance, DQ achieved a 99% reduction in mean squared error in large-scale commercial scenarios.</description></item></channel></rss>