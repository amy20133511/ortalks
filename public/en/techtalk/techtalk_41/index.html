<!doctype html><html lang=en dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><title>[Tech Talk #41] Columbia DRO Tianyi Peng: When A/B Testing Platforms Meet Reinforcement Learning :: ORAI China - Example site for hugo-theme-tailwind</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A novel &amp;ldquo;Difference-in-Q&amp;rdquo; (DQ) estimator, based on reinforcement learning, is proposed to address the Interference problem in A/B testing. DQ outperforms traditional estimators in bias-variance trade-off, reducing bias and exponentially decreasing variance. Collaborating with ByteDance, DQ achieved a 99% reduction in mean squared error in large-scale commercial scenarios."><meta name=keywords content="hugo,tailwind,tailwindcss,hugo theme,hugo theme tailwind"><meta name=robots content="noodp"><meta property="og:url" content="http://localhost:1313/en/techtalk/techtalk_41/"><meta property="og:site_name" content="ORAI China"><meta property="og:title" content="[Tech Talk #41] Columbia DRO Tianyi Peng: When A/B Testing Platforms Meet Reinforcement Learning"><meta property="og:description" content="A novel &amp;ldquo;Difference-in-Q&amp;rdquo; (DQ) estimator, based on reinforcement learning, is proposed to address the Interference problem in A/B testing. DQ outperforms traditional estimators in bias-variance trade-off, reducing bias and exponentially decreasing variance. Collaborating with ByteDance, DQ achieved a 99% reduction in mean squared error in large-scale commercial scenarios."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="techtalk"><meta property="article:published_time" content="2023-09-03T10:00:00+00:00"><meta property="article:modified_time" content="2023-09-03T10:00:00+00:00"><meta property="article:tag" content="A/B Testing"><meta property="article:tag" content="Experiment Design"><meta property="article:tag" content="Interference"><meta property="article:tag" content="Off-Policy Evaluation"><meta property="article:tag" content="Reinforcement Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="[Tech Talk #41] Columbia DRO Tianyi Peng: When A/B Testing Platforms Meet Reinforcement Learning"><meta name=twitter:description content="A novel &ldquo;Difference-in-Q&rdquo; (DQ) estimator, based on reinforcement learning, is proposed to address the Interference problem in A/B testing. DQ outperforms traditional estimators in bias-variance trade-off, reducing bias and exponentially decreasing variance. Collaborating with ByteDance, DQ achieved a 99% reduction in mean squared error in large-scale commercial scenarios."><link rel=canonical href=http://localhost:1313/en/techtalk/techtalk_41/><link rel="shortcut icon" href=/favicon.ico><link rel=stylesheet href=/css/index.7e915884f3dd6d57e42096f9ebaa8c2c1bce67d7b52260cc1f09af3ab6af714a.css></head><body class="flex flex-col min-h-screen w-full bg-slate-50 dark:bg-gray-800"><header class="flex flex-none justify-center z-10"><div class="flex flex-row gap justify-between w-full max-w-4xl lg:max-w-5xl h-12 mt-3"><div class="flex-none ml-2 md:ml-0"><a href=/en/><img class="h-12 w-12 rounded-full object-cover bg-gray-100" src=/favicon.ico alt=logo></a></div><div class=flex-1></div><div class=flex-none><nav class="h-full static"><button id=navbar-menu-toggle type=button class="inline-flex items-center p-2 text-sm text-slate-800 dark:text-slate-200 rounded-lg md:hidden" aria-controls=navbar-menu aria-expanded=false>
<span class=sr-only>Open main menu</span>
<i class="w-8 h-8"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-menu-2" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 6h16"/><path d="M4 12h16"/><path d="M4 18h16"/></svg></i></button><div class="absolute md:static top-16 left-0 right-0 z-50 hidden w-full md:block md:w-auto" id=navbar-menu><ul class="flex flex-col mx-2 md:mx-0 md:flex-row md:border-0 rounded-sm md:rounded-full px-3 text-base font-medium text-slate-800 dark:text-slate-200 shadow-lg bg-white dark:bg-gray-600 shadow-slate-800/5 dark:shadow-slate-200/5 ring-1 ring-slate-900/5 dark:ring-slate-100/5"><li id=ortalks><a class="block px-3 py-3 hover:text-emerald-600" href=/ortalk/ title="OR Talk">OR Talk</a></li><li id=techtalks><a class="block px-3 py-3 hover:text-emerald-600 text-emerald-600" href=/techtalk/ title="Tech Talk">Tech Talk</a></li><li id=intalks><a class="block px-3 py-3 hover:text-emerald-600" href=/intalk/ title="Industry Talk">Industry Talk</a></li></ul></div></nav></div><div class=flex-none><div class="h-full static"><button id=navbar-lang-toggle type=button class="inline-flex items-center p-2 text-sm text-slate-800 dark:text-slate-200 rounded-lg" aria-controls=navbar-menu aria-expanded=false>
<span class=sr-only>Open language switcher</span>
<i class="w-8 h-8"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c0 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg></i></button><div class="absolute hidden top-16 z-50" id=navbar-lang><ul class="flex flex-col rounded-sm px-3 text-base font-medium text-slate-800 dark:text-slate-200 shadow-lg bg-white dark:bg-gray-600 shadow-slate-800/5 dark:shadow-slate-200/5 ring-1 ring-slate-900/5 dark:ring-slate-100/5"><li><span class="block px-3 py-3 cursor-default hover:text-emerald-600">English</span></li></ul></div></div></div><div class="flex-none md:hidden"><a href=/en/search/ class="inline-flex items-center p-2 text-sm text-slate-800 dark:text-slate-200 rounded-lg" aria-controls=navbar-menu aria-expanded=false><span class=sr-only>Search</span>
<i class="w-8 h-8"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M10 10m-7 0a7 7 0 1014 0A7 7 0 103 10"/><path d="M21 21l-6-6"/></svg></i></a></div><div class="darkmode-toggle flex flex-none mr-2 md:mr-0"><label for=darkmode-toggle class="flex items-center px-3 cursor-pointer rounded-full bg-gray-100 dark:bg-gray-600" title="Toggle dark mode"><input name=darkmode-toggle id=darkmode-toggle type=checkbox class="sr-only peer" aria-label="Toggle dark mode"><div class="group flex flex-row gap-1 justify-center h-8 px-1 rounded-full bg-white dark:bg-gray-700"><i class="h-6 w-6 flex-none rounded-full bg-yellow-400 place-self-center peer-checked:group-[]:invisible"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brightness-down" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 12m-3 0a3 3 0 106 0 3 3 0 10-6 0"/><path d="M12 5v.01"/><path d="M17 7v.01"/><path d="M19 12v.01"/><path d="M17 17v.01"/><path d="M12 19v.01"/><path d="M7 17v.01"/><path d="M5 12v.01"/><path d="M7 7v.01"/></svg>
</i><i class="h-6 w-6 flex-none rounded-full place-self-center invisible peer-checked:group-[]:visible"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-moon-stars" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3c.132.0.263.0.393.0a7.5 7.5.0 007.92 12.446A9 9 0 1112 2.992z"/><path d="M17 4a2 2 0 002 2 2 2 0 00-2 2 2 2 0 00-2-2 2 2 0 002-2"/><path d="M19 11h2m-1-1v2"/></svg></i></div></label></div></div></header><main class="flex flex-auto justify-center"><div class="w-full max-w-4xl lg:max-w-5xl"><div class="flex flex-col gap-y-3 p-6 mt-6 mx-2 md:mx-0 rounded-lg shadow-md bg-white dark:bg-gray-700"><h1 class="text-4xl font-semibold text-slate-800 dark:text-slate-100"><a href=/en/techtalk/techtalk_41/>[Tech Talk #41] Columbia DRO Tianyi Peng: When A/B Testing Platforms Meet Reinforcement Learning</a></h1><ul class="flex flex-row flex-wrap text-slate-500 dark:text-slate-300"><li><a href=/en/categories/applied-statistics/ class="text-sm mr-2 px-2 py-1 rounded border border-emerald-800 bg-emerald-800 text-slate-50">Applied Statistics</a></li><li><a href=/en/tags/a/b-testing/ class="flex flex-row text-sm mr-2 py-1"><i class="h-5 w-5 flex-none"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 9h14"/><path d="M5 15h14"/><path d="M11 4 7 20"/><path d="M17 4l-4 16"/></svg>
</i><span class=ml-0>A/B testing</span></a></li><li><a href=/en/tags/experiment-design/ class="flex flex-row text-sm mr-2 py-1"><i class="h-5 w-5 flex-none"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 9h14"/><path d="M5 15h14"/><path d="M11 4 7 20"/><path d="M17 4l-4 16"/></svg>
</i><span class=ml-0>experiment design</span></a></li><li><a href=/en/tags/interference/ class="flex flex-row text-sm mr-2 py-1"><i class="h-5 w-5 flex-none"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 9h14"/><path d="M5 15h14"/><path d="M11 4 7 20"/><path d="M17 4l-4 16"/></svg>
</i><span class=ml-0>interference</span></a></li><li><a href=/en/tags/off-policy-evaluation/ class="flex flex-row text-sm mr-2 py-1"><i class="h-5 w-5 flex-none"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 9h14"/><path d="M5 15h14"/><path d="M11 4 7 20"/><path d="M17 4l-4 16"/></svg>
</i><span class=ml-0>off-policy evaluation</span></a></li><li><a href=/en/tags/reinforcement-learning/ class="flex flex-row text-sm mr-2 py-1"><i class="h-5 w-5 flex-none"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 9h14"/><path d="M5 15h14"/><path d="M11 4 7 20"/><path d="M17 4l-4 16"/></svg>
</i><span class=ml-0>reinforcement learning</span></a></li></ul><div class="flex flex-col gap-y-1 md:flex-row md:gap-y-0 md:gap-x-4 text-slate-500 dark:text-slate-300"><div class="flex flex-row text-base gap-x-1"><i class="h-6 w-6 flex-none"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 7a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2V7z"/><path d="M16 3v4"/><path d="M8 3v4"/><path d="M4 11h16"/><path d="M11 15h1"/><path d="M12 15v3"/></svg>
</i><time datetime=2023-09-03T10:00:00+00:00>2023-09-03</time></div><div class="flex flex-row text-base gap-x-1"><i class="h-6 w-6 flex-none"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hourglass-high" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M6.5 7h11"/><path d="M6 20v-2a6 6 0 1112 0v2a1 1 0 01-1 1H7a1 1 0 01-1-1z"/><path d="M6 4v2a6 6 0 1012 0V4a1 1 0 00-1-1H7A1 1 0 006 4z"/></svg>
</i><span>3 minutes to read</span></div></div><section class="prose prose-slate dark:prose-invert w-full max-w-4xl lg:max-w-5xl mt-6"><h2>Table of Contents</h2><aside><nav id=TableOfContents><ul><li><a href=#talk-abstract>Talk Abstract</a></li><li><a href=#about-the-speaker>About the Speaker</a></li><li><a href=#qa>Q&amp;A</a></li><li><a href=#livestream-video>Livestream Video</a></li><li><a href=#relevant-papers--recommended-reading>Relevant Papers & Recommended Reading</a><ul><li><a href=#past-talks--related-articles>Past Talks & Related Articles</a></li></ul></li></ul></nav></aside></section><article class="mt-6 w-full max-w-4xl lg:max-w-5xl prose prose-slate dark:prose-invert prose-quoteless post-content"><h2 id=talk-abstract>Talk Abstract</h2><p>In the current internet era, A/B testing has become the gold standard for evaluating algorithm performance. However, the issue of interference (the phenomenon where different experimental units affect each other) has consistently posed a significant challenge in A/B testing. This problem undermines the reliability of test results from experimental platforms, despite substantial investments from the industry. To address the interference issue, we propose an innovative solution based on a reinforcement learning framework to reassess A/B testing. This solution estimates the treatment effect by solving for the difference in Q-values in reinforcement learning, hence we name it the &ldquo;Difference-in-Q&rdquo; (DQ) estimator. Theoretically, we found that DQ performs exceptionally well in terms of the bias-variance trade-off: it significantly reduces the bias of traditional estimators while achieving exponential reductions in variance compared to any unbiased estimator. In collaboration with Douyin, we applied DQ to large-scale commercial scenarios, where preliminary tests showed a reduction in mean squared error by over 99%. Additionally, DQ demonstrated outstanding performance in a commercial-grade car-sharing simulator. In this presentation, I will introduce the theory and practice of DQ, and discuss the design and prospects of the next generation of intelligent experimental platforms.</p><h2 id=about-the-speaker>About the Speaker</h2><figure><img src=/tech_images/tech_41/speaker.png width=200><figcaption><h4>Tianyi Peng</h4></figcaption></figure><p>Assistant Professor (appointed) in the Decision, Risk, and Operations Division at Columbia Business School. He earned his Ph.D. from the Massachusetts Institute of Technology in 2023 and graduated from the Yao Class at Tsinghua University in 2017. Currently, he serves as the Chief AI Researcher at Cimulate.AI. His research interests focus on generative artificial intelligence, reinforcement learning, and causal inference. He enjoys exploring the application of cutting-edge theoretical problems to real-world issues and has collaborated with companies such as ByteDance, Anheuser-Busch, and the Broad Institute. He has received several awards, including the INFORMS Daniel H. Wagner Prize for Excellence in Operations Research Practice, the Applied Probability Society Best Student Prize, the RMP Jeff McGill Student Paper Award, and was a finalist for the MSOM Best Student Prize.</p><h2 id=qa>Q&amp;A</h2><dl><dt>Can A/B testing be extended to scenarios with multiple groups like A/B/C/D?</dt><dd>Yes, it can. The methodology is highly scalable, but it requires modifications to the corresponding mathematical descriptions.</dd><dt>In A/A testing, does the two A’s refer to comparisons between different versions?</dt><dd>No, A/A testing refers to split testing of the same algorithm. In this case, the true treatment effect is 0. It is generally used to check the robustness of the estimator or to estimate the variance of the estimator. A/A testing can divide users into two groups for testing, and sometimes it is also conducted across two different time periods.</dd><dt>What is the inspiration/intuition behind the DQ method?</dt><dd>From the perspective of reinforcement learning (RL), interference mainly arises because the current action affects future rewards. Therefore, our method to eliminate interference uses the sum of future rewards (i.e., Q-value) instead of the current reward to calculate the difference between the two experimental groups. We provided two examples in the Douyin scenario to further explain this, and for more details, you are welcome to watch the video or read the paper.</dd></dl><hr><h2 id=livestream-video>Livestream Video</h2><div class="w-full aspect-video"><iframe class="w-full aspect-video" src="//player.bilibili.com/player.html?bvid=BV11G411R7hR&p=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen></iframe></div><h2 id=relevant-papers--recommended-reading>Relevant Papers & Recommended Reading</h2><ul><li>Farias, Vivek, Andrew Li, Tianyi Peng, and Andrew Zheng. &ldquo;Markovian interference in experiments.&rdquo; <em>Advances in Neural Information Processing Systems</em> 35 (2022): 535-549.</li><li>Farias, Vivek F., Hao Li, Tianyi Peng, Xinyuyang Ren, Huawei Zhang, and Andrew Zheng. &ldquo;Correcting for Interference in Experiments: A Case Study at Douyin.&rdquo; *Accepted by RecSys 2023.</li></ul><hr><h3 id=past-talks--related-articles>Past Talks & Related Articles</h3><ul><li><a href=https://mp.weixin.qq.com/s/GSfAatYmLMcVna1Q12exHA target=_blank rel=noopener>Interview｜Rising Star in Operations Research and MIT PhD Student Peng Tianyi: Assisting the World&rsquo;s Largest Beer Production Group in Sales Decisions</a></li><li><a href=https://mp.weixin.qq.com/s/9TjC9NWeA492lpydyrDr2A target=_blank rel=noopener>OM | Meituan&rsquo;s A/B Evaluation System Construction and Practice</a></li><li><a href=https://mp.weixin.qq.com/s/qPPycUikDrHZFoGij3I-pg target=_blank rel=noopener>Introduction to Didi&rsquo;s Trading Strategies Part One: Trading Market</a></li></ul><hr></article></div></div></main><footer class="flex flex-none justify-center"><section class="flex flex-col md:flex-row mx-2 md:mx-0 gap-2 md:gap-0 justify-between w-full max-w-4xl lg:max-w-5xl py-6 text-slate-500 dark:text-slate-300"><div class="flex flex-row"></div><div class=grow></div><div class="flex flex-row"><i class="h-6 w-6 flex-none"><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 12m-9 0a9 9 0 1018 0A9 9 0 103 12"/><path d="M14 9.75a3.016 3.016.0 00-4.163.173 2.993 2.993.0 000 4.154A3.016 3.016.0 0014 14.25"/></svg>
</i>2018 - 2024 运筹OR帷幄</div></section></footer><script src=/main.js></script></body></html>